import os
import faiss
import pickle
import torch
from transformers import AutoTokenizer, AutoModel
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
import numpy as np

# ƒê∆∞·ªùng d·∫´n t·ªõi c√°c file
base_path = r"E:\Code\Master\BDT\Test\CloneData"
faiss_has_accent_path = os.path.join(base_path, "faiss_has_accent.index")
faiss_no_accent_path = os.path.join(base_path, "faiss_no_accent.index")
faiss_ids_path = os.path.join(base_path, "faiss_ids.pkl")
modelName = "bert-base-multilingual-cased"
# modelName = "vinai/phobert-large"

# K·∫øt n·ªëi MySQL b·∫±ng SQLAlchemy
def connect_db():
    engine = create_engine("mysql+pymysql://root:root@localhost/chatbot")
    return engine

# Load BERT model v√† tokenizer
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained(modelName)
model = AutoModel.from_pretrained(modelName).to(device)

# H√†m m√£ h√≥a vƒÉn b·∫£n th√†nh vector
def encode_text(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state[:, 0, :].cpu().numpy().astype(np.float32)

# K·∫øt n·ªëi DB
engine = connect_db()
Session = sessionmaker(bind=engine)
session = Session()

# FAISS index
embedding_dim = 768

# Load FAISS index n·∫øu c√≥, n·∫øu kh√¥ng t·∫°o m·ªõi
if os.path.exists(faiss_has_accent_path):
    index_has_accent = faiss.read_index(faiss_has_accent_path)
    print(f"üìÇ ƒê√£ load faiss_has_accent.index v·ªõi {index_has_accent.ntotal} vector")
else:
    index_has_accent = faiss.IndexFlatL2(embedding_dim)
    print("üìÅ T·∫°o m·ªõi faiss_has_accent.index")

if os.path.exists(faiss_no_accent_path):
    index_no_accent = faiss.read_index(faiss_no_accent_path)
    print(f"üìÇ ƒê√£ load faiss_no_accent.index v·ªõi {index_no_accent.ntotal} vector")
else:
    index_no_accent = faiss.IndexFlatL2(embedding_dim)
    print("üìÅ T·∫°o m·ªõi faiss_no_accent.index")

# Load danh s√°ch ID ƒë√£ l∆∞u n·∫øu c√≥
if os.path.exists(faiss_ids_path):
    with open(faiss_ids_path, "rb") as f:
        id_list = pickle.load(f)
    print(f"üìÇ ƒê√£ load faiss_ids.pkl v·ªõi {len(id_list)} ID")
else:
    id_list = []

# L·∫•y d·ªØ li·ªáu ch∆∞a vector h√≥a
result = session.execute(text("SELECT id, main_title, main_title_no_accent, content, content_no_accent FROM uet_clear WHERE vector = 0"))
total = result.rowcount
count = 0

for row in result:
    id_clear, main_title, main_title_no_accent, content, content_no_accent = row
    text_has_accent = f"{main_title} {content}" if main_title else content
    text_no_accent = f"{main_title_no_accent} {content_no_accent}" if main_title_no_accent else content_no_accent

    before_ha = index_has_accent.ntotal
    before_na = index_no_accent.ntotal

    vector_has_accent = encode_text(text_has_accent)
    vector_no_accent = encode_text(text_no_accent)
    index_has_accent.add(vector_has_accent)
    index_no_accent.add(vector_no_accent)

    after_ha = index_has_accent.ntotal
    after_na = index_no_accent.ntotal

    id_list.append(id_clear)
    session.execute(text("UPDATE uet_clear SET vector = 1 WHERE id = :id_clear"), {"id_clear": id_clear})
    session.commit()


    count += 1
    if count % 5000 == 0:
        # L∆∞u ngay sau m·ªói l·∫ßn th√™m vector
        faiss.write_index(index_has_accent, faiss_has_accent_path)
        faiss.write_index(index_no_accent, faiss_no_accent_path)
        with open(faiss_ids_path, "wb") as f:
            pickle.dump(id_list, f)
        print(f"‚úÖ {count}/{total} - ID {id_clear} | +1 vector (HA: {before_ha} ‚Üí {after_ha}, NA: {before_na} ‚Üí {after_na})")


faiss.write_index(index_has_accent, faiss_has_accent_path)
faiss.write_index(index_no_accent, faiss_no_accent_path)
with open(faiss_ids_path, "wb") as f:
    pickle.dump(id_list, f)

print("‚úÖ Ho√†n t·∫•t c·∫≠p nh·∫≠t v√† l∆∞u FAISS index! T·ªïng vector hi·ªán t·∫°i:")
print(f"   - C√≥ d·∫•u: {index_has_accent.ntotal}")
print(f"   - Kh√¥ng d·∫•u: {index_no_accent.ntotal}")
session.close()
