from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from bs4 import BeautifulSoup
import re
import unicodedata
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# K·∫øt n·ªëi MySQL b·∫±ng SQLAlchemy
def connect_db():
    engine = create_engine("mysql+pymysql://root:root@localhost/chatbot")
    return engine

# L√†m s·∫°ch HTML
def clean_html(raw_html):
    soup = BeautifulSoup(raw_html, "html.parser")
    text = soup.get_text()
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Lo·∫°i b·ªè d·∫•u ti·∫øng Vi·ªát
def remove_accents(text):
    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')

# Chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát
def normalize_text(text):
    text = text.lower()
    text = unicodedata.normalize('NFKC', text)
    text = re.sub(r'[^\w\s]', '', text)
    return text.strip()

# Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ MySQL
def fetch_paragraphs():
    engine = connect_db()
    Session = sessionmaker(bind=engine)
    session = Session()
    result = session.execute(text("SELECT id, paragraph FROM uet_content"))

    data = []
    for row in result:
        clean_text = clean_html(row[1])
        normalized_text = normalize_text(clean_text)
        no_accent_text = remove_accents(normalized_text)  # Phi√™n b·∫£n kh√¥ng d·∫•u
        data.append((row[0], normalized_text, no_accent_text))

    session.close()
    return data

# Kh·ªüi t·∫°o m√¥ h√¨nh SBERT
model = SentenceTransformer('bert-base-nli-mean-tokens')

# Chuy·ªÉn vƒÉn b·∫£n th√†nh embedding
def get_embedding(text):
    return model.encode(text)

# L∆∞u embeddings v√†o FAISS
def save_to_faiss(data):
    embeddings = np.array([get_embedding(text) for _, text, _ in data])
    index = faiss.IndexFlatL2(768)
    index.add(embeddings)
    faiss.write_index(index, "uet_vector.index")

# T√¨m ki·∫øm trong FAISS v√† hi·ªÉn th·ªã n·ªôi dung vƒÉn b·∫£n
def search(query, data, top_k=5):
    index = faiss.read_index("uet_vector.index")
    query_normalized = normalize_text(query)
    query_vector = get_embedding(query_normalized).reshape(1, -1)
    distances, indices = index.search(query_vector, top_k)

    results = []
    for idx in indices[0]:  # indices[0] ch·ª©a danh s√°ch index
        results.append(data[idx])  # L·∫•y ƒëo·∫°n vƒÉn b·∫£n theo index

    return results

if __name__ == "__main__":
    data = fetch_paragraphs()
    save_to_faiss(data)
    print("D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch v√† l∆∞u v√†o FAISS.")

    # Test t√¨m ki·∫øm
    query = "Th·ªùi gian ƒëƒÉng k√Ω m√¥n h·ªçc?"
    results = search(query, data)

    print("\nüîé K·∫øt qu·∫£ t√¨m ki·∫øm:")
    for r in results:
        print(f"üìå ID: {r[0]} - N·ªôi dung: {r[1]}")
