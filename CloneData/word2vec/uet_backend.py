import pickle  # ‚úÖ Th√™m import pickle
import torch
import faiss  # ‚úÖ Th√™m FAISS
import numpy as np
from transformers import BertModel, BertTokenizer, BertForQuestionAnswering, AutoTokenizer
from sqlalchemy import create_engine, text
from flask import Flask, request, jsonify
from flask_cors import CORS  # ‚úÖ Th√™m CORS ƒë·ªÉ fix l·ªói k·∫øt n·ªëi React

app = Flask(__name__)
CORS(app)  # ‚úÖ Cho ph√©p frontend k·∫øt n·ªëi v·ªõi backend

modelName = "bert-base-multilingual-uncased"

# K·∫øt n·ªëi MySQL
def connect_db():
    engine = create_engine("mysql+pymysql://root:root@localhost/chatbot")
    return engine

# Load FAISS index
def load_faiss_index():
    index_has_accent = faiss.read_index(r"E:\\Code\\Master\\BDT\\Test\\CloneData\\faiss_has_accent.index")
    index_no_accent = faiss.read_index(r"E:\\Code\\Master\\BDT\\Test\\CloneData\\faiss_no_accent.index")
    with open(r"E:\\Code\\Master\\BDT\\Test\\CloneData\\faiss_ids.pkl", "rb") as f:
        id_list = pickle.load(f)  # üî• ƒê·∫£m b·∫£o danh s√°ch ID ƒë∆∞·ª£c load ƒë√∫ng
    return index_has_accent, index_no_accent, id_list

# Encode vƒÉn b·∫£n th√†nh vector v·ªõi BERT
def encode_text(text, tokenizer, model, device):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state[:, 0, :].cpu().numpy().astype(np.float32)

# T√¨m ki·∫øm trong FAISS
def search_faiss(query, index, tokenizer, model, device, top_k):
    query_vector = encode_text(query, tokenizer, model, device)
    distances, indices = index.search(query_vector, top_k)
    return indices[0]

# Load m√¥ h√¨nh BERT QA ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi
qa_tokenizer = BertTokenizer.from_pretrained(modelName)
qa_model = BertForQuestionAnswering.from_pretrained(modelName).to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))

# Sinh c√¢u tr·∫£ l·ªùi t·ª´ BERT
def generate_answer(question, context):
    inputs = qa_tokenizer(question, context, return_tensors="pt", padding=True, truncation=True, max_length=512).to(qa_model.device)
    with torch.no_grad():
        outputs = qa_model(**inputs)

    start_scores, end_scores = outputs.start_logits, outputs.end_logits
    start_index = torch.argmax(start_scores)
    end_index = torch.argmax(end_scores) + 1

    answer = qa_tokenizer.convert_tokens_to_string(
        qa_tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][start_index:end_index])
    )

    return answer

# Load BERT model v√† tokenizer
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = BertTokenizer.from_pretrained(modelName)
model = BertModel.from_pretrained(modelName).to(device)

# K·∫øt n·ªëi DB v√† FAISS
engine = connect_db()
session = engine.connect()
index_has_accent, index_no_accent, id_list = load_faiss_index()

@app.route("/chatbot", methods=["POST"])
def chatbot():
    data = request.json
    query = data.get("question")
    
    # T√¨m ki·∫øm trong FAISS
    indices = search_faiss(query, index_has_accent, tokenizer, model, device, top_k=5)
    
    # Truy v·∫•n n·ªôi dung t·ª´ MySQL
    retrieved_texts = []
    for idx in indices:
        if 0 <= idx < len(id_list):
            doc_id = id_list[idx]
            query_db = text("SELECT main_title, content FROM uet_clear WHERE id = :id")
            row = session.execute(query_db, {"id": doc_id}).fetchone()
            if row:
                retrieved_texts.append(f"{row[0]}: {row[1]}")

    # G·ªôp n·ªôi dung l·∫°i l√†m ng·ªØ c·∫£nh
    context = " ".join(retrieved_texts)
    
    if context:
        answer = generate_answer(query, context)
        return jsonify({"answer": answer, "indices": ",".join(map(str, indices.tolist()))})
    else:
        return jsonify({"answer": "Kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi ph√π h·ª£p."})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)
