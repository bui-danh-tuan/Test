import datetime
import scrapy
import sqlite3
import os
import re
from bs4 import BeautifulSoup  # Import BeautifulSoup ƒë·ªÉ x·ª≠ l√Ω HTML

class ContentSpider(scrapy.Spider):
    name = "uet_content"

    def __init__(self):
        """Kh·ªüi t·∫°o CSDL SQLite v√† t·∫°o b·∫£ng n·∫øu ch∆∞a t·ªìn t·∫°i."""
        self.conn = sqlite3.connect("scrapy.db")  # K·∫øt n·ªëi ƒë·∫øn SQLite
        self.cursor = self.conn.cursor()

        # L·∫•y danh s√°ch URL c·∫ßn thu th·∫≠p
        self.cursor.execute("SELECT url FROM uet_url WHERE content = 0")
        self.start_urls = [row[0] for row in self.cursor.fetchall()]

        # T·∫°o b·∫£ng uet_content n·∫øu ch∆∞a t·ªìn t·∫°i
        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS uet_content (
                url TEXT,
                paragraph TEXT,
                type TEXT CHECK(type IN ('url', 'file')),  -- Lo·∫°i d·ªØ li·ªáu: 'url' ho·∫∑c 'file'
                last_modified DATE  -- Th·ªùi gian c·∫≠p nh·∫≠t
            )
        """)
        self.conn.commit()

    def updateContent(self, page_url, content, content_type):
        """C·∫≠p nh·∫≠t n·ªôi dung v√†o b·∫£ng uet_content"""
        self.cursor.execute("""
            INSERT INTO uet_content (url, paragraph, type, last_modified) 
            VALUES (?, ?, ?, ?)""",
            (page_url, content, content_type, datetime.datetime.now())
        )
        self.conn.commit()
        
    def updateUrl(self, url):
        """ƒê√°nh d·∫•u URL ƒë√£ ƒë∆∞·ª£c thu th·∫≠p n·ªôi dung"""
        self.cursor.execute("UPDATE uet_url SET content = 1, last_modified = ? WHERE url = ?", 
                            (datetime.datetime.now(), url))
        self.conn.commit()

    def parse(self, response):
        content_type = response.headers.get('Content-Type', b'').decode()
        page_url = response.url  # L·∫•y URL c·ªßa trang
        save_dir = r"E:\Code\Master\BDT\CloneFile"  # Th∆∞ m·ª•c l∆∞u file

        if 'text/html' in content_type:
            print(f"‚úÖ ƒêang x·ª≠ l√Ω HTML: {page_url}")

            # S·ª≠ d·ª•ng BeautifulSoup ƒë·ªÉ ph√¢n t√≠ch HTML
            soup = BeautifulSoup(response.text, "html.parser")

            # X√≥a c√°c th·∫ª <script> v√† <style>
            for tag in soup(["script", "style"]):
                tag.decompose()

            # L·∫•y n·ªôi dung HTML ƒë√£ l·ªçc b·ªè JS & CSS
            cleaned_html = str(soup)

            # L∆∞u v√†o SQLite v·ªõi type='url'
            self.updateContent(page_url, cleaned_html, "url")
            self.updateUrl(page_url)

            print(f"‚úÖ ƒê√£ l·∫•y n·ªôi dung t·ª´ {page_url} (Kh√¥ng c√≥ JS & CSS)")

        else:
            print(f"üìÇ L∆∞u file t·ª´ {page_url} (Lo·∫°i: {content_type})")

            # Chuy·ªÉn ƒë·ªïi URL th√†nh t√™n file h·ª£p l·ªá
            safe_filename = re.sub(r'[<>:"/\\|?*]', '_', page_url)  

            # ƒê·ªãnh d·∫°ng file d·ª±a v√†o Content-Type
            ext = {
                'application/pdf': '.pdf',
                'image/jpeg': '.jpg',
                'image/png': '.png',
                'image/gif': '.gif',
                'application/zip': '.zip',
                'application/msword': '.doc',
                'application/vnd.openxmlformats-officedocument.wordprocessingml.document': '.docx',
            }.get(content_type, '')  # M·∫∑c ƒë·ªãnh kh√¥ng c√≥ ƒëu√¥i

            filename = f"{safe_filename}{ext}"  
            filepath = os.path.join(save_dir, filename)

            # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i
            os.makedirs(save_dir, exist_ok=True)

            # L∆∞u file v√†o ·ªï ƒëƒ©a
            with open(filepath, "wb") as f:
                f.write(response.body)

            # L∆∞u v√†o SQLite v·ªõi type='file' v√† paragraph l√† ƒë∆∞·ªùng d·∫´n file
            self.updateContent(page_url, filepath, "file")
            self.updateUrl(page_url)

            print(f"‚úÖ File ƒë√£ l∆∞u: {filepath}")

    def closed(self, reason):
        """ƒê√≥ng k·∫øt n·ªëi SQLite sau khi thu th·∫≠p d·ªØ li·ªáu xong."""
        self.conn.close()
